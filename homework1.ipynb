
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install torchvision==0.6.1\n",
    "clear_output()\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "DATA_PATH = '/root/userspace/public/day1/homework1/data/'\n",
    "\n",
    "train = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "train = train.fillna(train.median()).values\n",
    "x_train = train[:, :-1]\n",
    "t_train = train[:, -1]\n",
    "\n",
    "x_test = pd.read_csv(DATA_PATH + 'test.csv')\n",
    "x_test = x_test.fillna(x_test.median()).values\n",
    "\n",
    "x_train, x_valid, t_train, t_valid = \\\n",
    "    train_test_split(x_train, t_train, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_valid = scaler.transform(x_valid)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, x_train, t_train):\n",
    "        self.x_train = x_train\n",
    "        self.t_train = t_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_train)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x_train[idx], dtype=torch.float), \\\n",
    "            torch.tensor(self.t_train[idx], dtype=torch.float)\n",
    "\n",
    "    \n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, x_test):\n",
    "        self.x_test = x_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x_test)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.x_test[idx], dtype=torch.float)\n",
    "\n",
    "\n",
    "train_dataset = TrainDataset(x_train, t_train)\n",
    "valid_dataset = TrainDataset(x_valid, t_valid)\n",
    "test_dataset = TestDataset(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットによる赤ワイン品質の回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.MLP'>\n",
      "Epoch 001: | Train Loss: 12.33593 | Val Loss: 2.52964\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 002: | Train Loss: 1.32969 | Val Loss: 0.89618\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 003: | Train Loss: 0.93596 | Val Loss: 0.80768\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 004: | Train Loss: 0.93716 | Val Loss: 0.84817\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 005: | Train Loss: 0.90976 | Val Loss: 0.84713\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 006: | Train Loss: 0.88325 | Val Loss: 0.80585\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 007: | Train Loss: 0.88394 | Val Loss: 0.85549\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 008: | Train Loss: 0.84205 | Val Loss: 0.80709\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 009: | Train Loss: 0.85977 | Val Loss: 0.79069\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 010: | Train Loss: 0.82688 | Val Loss: 0.83141\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 011: | Train Loss: 0.91728 | Val Loss: 0.78703\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 012: | Train Loss: 0.86400 | Val Loss: 0.79910\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 013: | Train Loss: 0.86151 | Val Loss: 0.82129\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 014: | Train Loss: 0.83624 | Val Loss: 0.79338\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 015: | Train Loss: 0.85667 | Val Loss: 0.77446\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 016: | Train Loss: 0.82160 | Val Loss: 0.78596\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 017: | Train Loss: 0.82975 | Val Loss: 0.78676\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 018: | Train Loss: 0.79946 | Val Loss: 0.77673\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 019: | Train Loss: 0.81992 | Val Loss: 0.79001\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 020: | Train Loss: 0.79371 | Val Loss: 0.77100\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 021: | Train Loss: 0.79477 | Val Loss: 0.77201\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 022: | Train Loss: 0.80845 | Val Loss: 0.78814\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 023: | Train Loss: 0.79972 | Val Loss: 0.76355\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 024: | Train Loss: 0.82327 | Val Loss: 0.76836\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 025: | Train Loss: 0.81535 | Val Loss: 0.77287\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 026: | Train Loss: 0.78036 | Val Loss: 0.76549\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 027: | Train Loss: 0.76049 | Val Loss: 0.76483\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 028: | Train Loss: 0.78303 | Val Loss: 0.76618\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 029: | Train Loss: 0.81190 | Val Loss: 0.76643\n",
      "\n",
      " learning rate  0.001\n",
      "Epoch 030: | Train Loss: 0.77849 | Val Loss: 0.76323\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 031: | Train Loss: 0.77244 | Val Loss: 0.77662\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 032: | Train Loss: 0.73281 | Val Loss: 0.76853\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 033: | Train Loss: 0.74223 | Val Loss: 0.76890\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 034: | Train Loss: 0.74570 | Val Loss: 0.76662\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 035: | Train Loss: 0.74716 | Val Loss: 0.77288\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 036: | Train Loss: 0.72788 | Val Loss: 0.76822\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 037: | Train Loss: 0.73619 | Val Loss: 0.76520\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 038: | Train Loss: 0.74644 | Val Loss: 0.76749\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 039: | Train Loss: 0.74781 | Val Loss: 0.76258\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 040: | Train Loss: 0.73137 | Val Loss: 0.75878\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 041: | Train Loss: 0.74417 | Val Loss: 0.76187\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 042: | Train Loss: 0.74187 | Val Loss: 0.76017\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 043: | Train Loss: 0.74234 | Val Loss: 0.76349\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 044: | Train Loss: 0.74557 | Val Loss: 0.75980\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 045: | Train Loss: 0.70663 | Val Loss: 0.76216\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 046: | Train Loss: 0.72486 | Val Loss: 0.77495\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 047: | Train Loss: 0.71233 | Val Loss: 0.75774\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 048: | Train Loss: 0.70435 | Val Loss: 0.75945\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 049: | Train Loss: 0.70843 | Val Loss: 0.75437\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 050: | Train Loss: 0.72349 | Val Loss: 0.75828\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 051: | Train Loss: 0.74871 | Val Loss: 0.76189\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 052: | Train Loss: 0.69732 | Val Loss: 0.77540\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 053: | Train Loss: 0.72274 | Val Loss: 0.76758\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 054: | Train Loss: 0.70963 | Val Loss: 0.77475\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 055: | Train Loss: 0.71171 | Val Loss: 0.75511\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 056: | Train Loss: 0.71407 | Val Loss: 0.75811\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 057: | Train Loss: 0.68927 | Val Loss: 0.76218\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 058: | Train Loss: 0.70323 | Val Loss: 0.75412\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 059: | Train Loss: 0.70405 | Val Loss: 0.75746\n",
      "\n",
      " learning rate  0.0006\n",
      "Epoch 060: | Train Loss: 0.72211 | Val Loss: 0.74606\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 061: | Train Loss: 0.68467 | Val Loss: 0.74706\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 062: | Train Loss: 0.68130 | Val Loss: 0.75319\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 063: | Train Loss: 0.67843 | Val Loss: 0.74970\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 064: | Train Loss: 0.69201 | Val Loss: 0.75504\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 065: | Train Loss: 0.67229 | Val Loss: 0.75234\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 066: | Train Loss: 0.67110 | Val Loss: 0.74857\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 067: | Train Loss: 0.70345 | Val Loss: 0.75137\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 068: | Train Loss: 0.68448 | Val Loss: 0.75447\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 069: | Train Loss: 0.66117 | Val Loss: 0.74661\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 070: | Train Loss: 0.68968 | Val Loss: 0.74989\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 071: | Train Loss: 0.66805 | Val Loss: 0.75032\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 072: | Train Loss: 0.66054 | Val Loss: 0.75091\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 073: | Train Loss: 0.66484 | Val Loss: 0.74506\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 074: | Train Loss: 0.68419 | Val Loss: 0.74593\n",
      "\n",
      " learning rate  0.00035999999999999997\n",
      "Epoch 075: | Train Loss: 0.66881 | Val Loss: 0.75153\n",
      "\n",
      " learning rate  0.00035999999999999997\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 精度向上ポイント: バッチサイズの大小\n",
    "BATCH_SIZE = 32\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, \n",
    "                                           batch_size=BATCH_SIZE)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=BATCH_SIZE)\n",
    "\n",
    "# ニューラルネットの定義\n",
    "# 精度向上ポイント: 中間層の数・ユニット数の大小、活性化関数の選択\n",
    "# 精度向上ポイント（発展）: ドロップアウト層・バッチノーマリゼーションの使用\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(num_features, 256)  # 入力層\n",
    "        self.layer_2 = nn.Linear(256, 512)  # 中間層\n",
    "        self.layer_3 = nn.Linear(512, 256)  # 中間層\n",
    "        self.layer_out = nn.Linear(256, 1)  # 出力層\n",
    "        \n",
    "        \n",
    "        self.dropout1 = nn.Dropout(p=0.3)\n",
    "        self.dropout2 = nn.Dropout(p=0.3)\n",
    "        self.dropout3 = nn.Dropout(p=0.3)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(256)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(256)\n",
    "        \n",
    "    def forward(self, inputs):     \n",
    "        x = F.relu(self.dropout1((self.layer_1(inputs))))\n",
    "        x = F.relu(self.dropout2(self.batchnorm2(self.layer_2(x))))\n",
    "        \n",
    "        x = torch.sigmoid(self.batchnorm3(self.layer_3(x)))\n",
    "        x = self.dropout3(x)\n",
    "#         x = F.relu(self.dropout3(self.batchnorm3(self.layer_3(x))))\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        return x\n",
    "\n",
    "print(MLP)\n",
    "\n",
    "NUM_FEATURES = 11\n",
    "mlp = MLP(NUM_FEATURES)\n",
    "mlp.to(DEVICE)\n",
    "# 損失関数の定義\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "    # オプティマイザの定義\n",
    "    # 精度向上ポイント: 学習率の選択、オプティマイザの選択\n",
    "    # 精度向上ポイント（発展）: weight decayの大小、スケジューラの使用\n",
    "LEARNING_RATE = 1e-3\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=LEARNING_RATE,weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer,step_size=30,gamma=0.6)\n",
    "\n",
    "    # 学習の実行\n",
    "    # 精度向上ポイント: エポック数の大小\n",
    "NUM_EPOCHS =75\n",
    "#75 best \n",
    "loss_stats = {'train': [], 'valid': []}\n",
    "loss_stats = {'train': [], 'valid': []}\n",
    "for e in range(1, NUM_EPOCHS+1):\n",
    "    \n",
    "    # 訓練\n",
    "    train_epoch_loss = 0\n",
    "    mlp.train()\n",
    "    for x, t in train_loader:\n",
    "        x, t = x.to(DEVICE), t.unsqueeze(1).to(DEVICE)\n",
    "        optimizer.zero_grad()  # 勾配の初期化\n",
    "        pred = mlp(x)  # 予測の計算(順伝播)\n",
    "        loss = loss_fn(pred, t)  # 損失関数の計算\n",
    "        loss.backward()  # 勾配の計算（逆伝播）\n",
    "        optimizer.step()  # 重みの更新\n",
    "        train_epoch_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    # 検証  \n",
    "    with torch.no_grad():\n",
    "        valid_epoch_loss = 0\n",
    "        mlp.eval()\n",
    "        for x, t in valid_loader:\n",
    "            x, t = x.to(DEVICE), t.unsqueeze(1).to(DEVICE)\n",
    "            pred = mlp(x)  # 予測の計算(順伝播)\n",
    "            loss = loss_fn(pred, t)  # 損失関数の計算\n",
    "            valid_epoch_loss += loss.item()\n",
    "            \n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['valid'].append(valid_epoch_loss/len(valid_loader))                              \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} \\\n",
    "| Val Loss: {valid_epoch_loss/len(valid_loader):.5f}')\n",
    "    print('\\n learning rate ',optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "\n",
    "# 学習曲線の描画\n",
    "df_loss = pd.DataFrame.from_dict(loss_stats).reset_index()\n",
    "df_loss = df_loss.melt(id_vars=['index']).rename(columns={\"index\": \"epochs\"})\n",
    "sns.lineplot(data=df_loss, x=\"epochs\", y=\"value\", hue=\"variable\")\n",
    "plt.gca().get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "\n",
    "\n",
    "# 推論の実行\n",
    "mlp.eval()\n",
    "preds = []\n",
    "for x in test_loader:\n",
    "    x = x.to(DEVICE)\n",
    "    pred = mlp(x)\n",
    "    pred = pred.squeeze()\n",
    "    preds.extend(pred.tolist())\n",
    "\n",
    "submission = pd.Series(preds, name='quality')\n",
    "submission.to_csv('/root/userspace/submission1_pred.csv', \n",
    "                  header=True, index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "chap05_homework_pytorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
